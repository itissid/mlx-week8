{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n",
      "/workspace/mlx-week8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(os.getcwd())\n",
    "# home = os.path.expanduser(\"~\")\n",
    "# home_path = Path(home)\n",
    "# print(home_path)\n",
    "project_path = Path(\"/workspace/mlx-week8\")\n",
    "# project_path = Path(os.path.expanduser(\"~\") + \"/workspace/mlx-week8\")\n",
    "print(project_path)\n",
    "os.chdir(project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert the data to pandas and filter them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save all of these once and reuse them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url_id_to_doctext_mapping = load(\"train_url_id_to_doctext_mapping.pkl\")\n",
    "test_url_id_to_doctext_mapping = load(\"test_url_id_to_doctext_mapping.pkl\")\n",
    "train_query_id_to_query_mapping = load(\"train_query_id_to_query_mapping.pkl\")\n",
    "test_query_id_to_query_mapping = load(\"test_query_id_to_query_mapping.pkl\")\n",
    "train_query_id_to_wfa = load(\"train_query_id_to_wfa.pkl\")\n",
    "test_query_id_to_wfa = load(\"test_query_id_to_wfa.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Batches of data\n",
    "\n",
    "### High Level:\n",
    "  - Follow a generate -> serialize strategy to generate all the embeddings in FAISS for passages, step by step.\n",
    "  - Create a template for the forward pass method to include the extra dimension for TopK docs.\n",
    "\n",
    "0. Look up generation[DONE]\n",
    "1. Embedding generation in memory for DOCS[DONE]\n",
    "The noted files saved on disk are:\n",
    "```\n",
    "137M    test_encoded_passage_attention_mask.pt\n",
    "137M    test_encoded_passage_tokens.pt \n",
    "173M    test_passage_embeddings.pt              -> TESTING: Embeddings for MARCO MS test set\n",
    "504K    test_query_id_to_query_mapping.pkl      -> TESTING: Query ID -> Query Text mapping\n",
    "1.2M    test_query_id_to_wfa.pkl                -> TESTING: QueryID to well formed answer mapping\n",
    "21M     test_url_id_to_doctext_mapping.pkl      -> TESTING: URL -> Passage Text mapping\n",
    "1.8G    train_encoded_passage_attention_mask.pt \n",
    "1.8G    train_encoded_passage_tokens.pt\n",
    "6.1M    train_query_id_to_query_mapping.pkl    -> TRAINING: Query ID -> Query Text mapping\n",
    "13M     train_query_id_to_wfa.pkl              -> TRAINING: QueryID to well formed answer mapping \n",
    "254M    train_url_id_to_doctext_mapping.pkl   -> TRAINING: URL -> Passage Text mapping\n",
    "1.8G    train_passage_embeddings.pt         -> TRAINING: Embeddings for MARCO MS train set\n",
    "```\n",
    "2. I have to figure out how to create a batch of size B * T tokens to then encoded which are padded. \n",
    "NOTE: I did not use a collator the last time around for preparing my data for the tiny stories task. But I might this time.\n",
    "\n",
    "  -  Data -> Dataset. I need to retrieve the doc IDs only during training time for getting the embeddings which would be B size vector with teh doc id of each passage. Same for the query IDs which I would use to retrive the query embeddings. Here would be the outoput from the torch dataset `__getitem__` method.\n",
    "\n",
    "```\n",
    "{\n",
    "    \"query_id\":...\n",
    "    # NOTE: these are Top K passage ids for the query\n",
    "    \"passage_ids\":[], # We don't need anything the well formed answer because its 1:1 mapped to query_id.\n",
    "}\n",
    "```\n",
    "\n",
    "  - Pre Generate all the emebedings for FAISS[IN_PROGRESS]\n",
    "    - Keep all the embeddings with faiss in memory should not be that big, but lets test this.\n",
    "  - Create the dataset and the loader to do this[TODO]\n",
    "  - We can load the embeddings for the top K passages per the query from the specific query in the `__getitem__` method of the dataset return those passage IDs. \n",
    "  - I'll need to save `*_query_id_to_query_mapping`, `*_url_id_to_doctext_mapping` and `*_query_id_to_wfa`\n",
    "        giving me access to all three things i need to make the dataset.\n",
    "  -\n",
    "\n",
    "\n",
    "3. The forward pass over the dataloader should load all the passages \n",
    "    -  In the loop:\n",
    "        - Lets do the tokenization and padding in the loop using the convenient huggingface interface,\n",
    "        this allows for:\n",
    "          ```\n",
    "          decoder_inputs = tokenizer(target_texts, padding=\"longest\", return_tensors=\"pt\")\n",
    "          encoder_inputs = tokenizer(source_texts, padding=\"longest\", return_tensors=\"pt\")\n",
    "          outputs = model(input_ids=encoder_inputs['input_ids'],\n",
    "                attention_mask=encoder_inputs['attention_mask'],\n",
    "                decoder_input_ids=decoder_inputs['input_ids'],\n",
    "                decoder_attention_mask=decoder_inputs['attention_mask'])\n",
    "          ```\n",
    "        - The query embeddings will need to be generated on the fly from the tunable model.I think we will use the Pipeline \n",
    "        - This will distribute the load of the task between the dataloader and the model's forward pass.\n",
    "\n",
    "## Forward pass\n",
    "NEXT: Feed the query and concatenated passages to the encoder of BART model.\n",
    "- [TODO]: FOllow ardavan's diagram to get the loss propagating correctly. Whats the advantage of hugging face's pipeline?\n",
    "```\n",
    "for batch in data_loader:\n",
    "  query_ids = batch['query_id']\n",
    "  passage_ids = batch['passage_ids']\n",
    "  with torch.no_grad():\n",
    "    query_embeddings = get_query_embeddings(query_ids, query_bert_model)\n",
    "    passage_embeddings = get_query_embeddings(query_ids, passage_bert_model)\n",
    "```\n",
    "\n",
    "\n",
    "NEXT: Get the embeddings for the query and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaModel,\n",
    "    BartModel,\n",
    "    RobertaTokenizer,\n",
    "    BartTokenizer,\n",
    "    RobertaTokenizerFast,\n",
    ")\n",
    "\n",
    "# Load RoBERTa\n",
    "# roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# roberta_tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "# norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "# normalized_embeddings = embeddings / norms\n",
    "dimension = 768\n",
    "\n",
    "# Create FAISS index for inner product (cosine similarity)\n",
    "index = faiss.IndexIDMap(faiss.IndexFlatL2(dimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets get all the embeddings\n",
    "# from transformers import RobertaModel, BartModel, RobertaTokenizer, BartTokenizer\n",
    "\n",
    "# Load RoBERTa\n",
    "# roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "# roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 30.  53. 106. 129.]\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "len(train_url_id_to_doctext_mapping),\n",
    "print(\n",
    "    np.quantile(\n",
    "        [len(v.split()) for v in train_url_id_to_doctext_mapping.values()],\n",
    "        [0.05, 0.5, 0.95, 0.99],\n",
    "    )\n",
    ")\n",
    "print(max([len(v.split()) for v in train_url_id_to_doctext_mapping.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'Ġmy', 'Ġdog', 'Ġis', 'Ġcute']\n",
      "tensor([[    0, 31414,     6,   127,  2335,    16, 11962,     2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s>Hello, my dog is cute</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = roberta_tokenizer.tokenize(\"Hello, my dog is cute\")\n",
    "print(tokens)\n",
    "tokens = roberta_tokenizer.encode(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "print(tokens)\n",
    "roberta_tokenizer.decode(tokens[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super fast tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': [0, 2387, 766, 16, 41906, 38, 173, 23, 10725, 1000, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]},\n",
       " True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = roberta_tokenizer(\"My name is john I work at MLX\")\n",
    "encoded, encoded.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<s>',\n",
       "  'My',\n",
       "  'Ġname',\n",
       "  'Ġis',\n",
       "  'Ġjohn',\n",
       "  'ĠI',\n",
       "  'Ġwork',\n",
       "  'Ġat',\n",
       "  'ĠML',\n",
       "  'X',\n",
       "  '</s>'],\n",
       " [None, 0, 1, 2, 3, 4, 5, 6, 7, 7, None])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(encoded.tokens(), encoded.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_passages = list(train_url_id_to_doctext_mapping.values())\n",
    "all_test_passages = list(test_url_id_to_doctext_mapping.values())\n",
    "# all_tokens = roberta_tokenizer.batch_encode_plus(all_passages, return_tensors='pt', padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roberta_tokenizer.is_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.mlx_week8 import data_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(data_utils)\n",
    "\n",
    "\n",
    "def tokenize_passages(passages):\n",
    "    return roberta_tokenizer(\n",
    "        passages, padding=True, truncation=False, return_tensors=\"pt\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = True\n",
    "if not tokenized:\n",
    "    encoded_inputs = tokenize_passages(all_passages)\n",
    "    torch.save(encoded_inputs[\"input_ids\"], \"train_encoded_passage_tokens.pt\")\n",
    "    torch.save(\n",
    "        encoded_inputs[\"attention_mask\"], \"train_encoded_passage_attention_mask.pt\"\n",
    "    )\n",
    "else:\n",
    "    pass  # Load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "tokenized = True\n",
    "if not tokenized:\n",
    "    # encoded_inputs = tokenize_passages(all_passages)\n",
    "    encoded_test_inputs = tokenize_passages(all_test_passages)\n",
    "    torch.save(encoded_test_inputs[\"input_ids\"], \"test_encoded_passage_tokens.pt\")\n",
    "    torch.save(\n",
    "        encoded_test_inputs[\"attention_mask\"], \"test_encoded_passage_attention_mask.pt\"\n",
    "    )\n",
    "else:\n",
    "    pass  # Load it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-generate the embeddings for the passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test passage embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(data_utils)\n",
    "\n",
    "if tokenized:\n",
    "    encoded_test_inputs = {\n",
    "        \"input_ids\": torch.load(\"test_encoded_passage_tokens.pt\"),\n",
    "        \"attention_mask\": torch.load(\"test_encoded_passage_attention_mask.pt\"),\n",
    "    }\n",
    "generated_embeddings = True\n",
    "if not generated_embeddings:\n",
    "    test_embeddings = data_utils.generate_embeddings(\n",
    "        {\n",
    "            \"input_ids\": encoded_test_inputs[\"input_ids\"],\n",
    "            \"attention_mask\": encoded_test_inputs[\"attention_mask\"],\n",
    "        },\n",
    "        batch_size=512, \n",
    "        device=\"cuda\",\n",
    "    )\n",
    "    torch.save(test_embeddings, \"test_passage_embeddings.pt\")\n",
    "    print(test_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Passage Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 33%|███▎      | 217/655 [16:57<34:40,  4.75s/it]"
     ]
    }
   ],
   "source": [
    "if tokenized:\n",
    "    encoded_train_inputs = {\n",
    "        \"input_ids\": torch.load(\"train_encoded_passage_tokens.pt\", map_location=device),\n",
    "        \"attention_mask\": torch.load(\"train_encoded_passage_attention_mask.pt\", map_location=device),\n",
    "    }\n",
    "generated_embeddings = False\n",
    "\n",
    "if not generated_embeddings:\n",
    "    train_embeddings = data_utils.generate_embeddings(\n",
    "        encoded_train_inputs, batch_size=1024, device=\"cuda\"\n",
    "    )\n",
    "    torch.save(train_embeddings, \"train_passage_embeddings.pt\")\n",
    "    print(train_embeddings.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "ids = np.array(range(len(train_url_id_to_doctext_mapping)), dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add embeddings along with their IDs\n",
    "index.add_with_ids(embeddings, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have the tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
